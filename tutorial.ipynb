{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1c24945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ 1.12.1\n",
      "NUM_GPUS 0\n",
      "transformers.__version__ 4.21.1\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gpt2']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "#data manupulation libs\n",
    "import numpy as np\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "print('torch.__version__', torch.__version__)\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print('NUM_GPUS', NUM_GPUS)\n",
    "\n",
    "# huggingface transformers\n",
    "import transformers\n",
    "print('transformers.__version__', transformers.__version__)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import GPTJForCausalLM\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "#fara\n",
    "from fara.ml.heads import GPT2HeadWithValueModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "local_hugface_directory = '/Users/carson/projects/modelstates/hugface_models/'\n",
    "#local_hugface_directory = '/home/carson/modelstates/hugface_models/'\n",
    "os.listdir(local_hugface_directory )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9bc86",
   "metadata": {},
   "source": [
    "Some model sizes\n",
    "\n",
    "'gpt2' num_params 124_439_808\n",
    "\n",
    "'gpt2' with value head num_params 124_440_577\n",
    "\n",
    "'gpt2-medium' num_params 354_823_168\n",
    "\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/configuration_utils.py#L54\n",
    "\n",
    "When you call `GPT2LMHeadModel.from_pretrained()`\n",
    "\n",
    "```\n",
    "class GPT2HeadWithValueModel(GPT2PreTrainedModel):\n",
    "\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        config.num_labels = 1\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "```\n",
    "\n",
    "the <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> config dict looks something like this:\n",
    "\n",
    "```\n",
    "GPT2Config {\n",
    "  \"_name_or_path\": \"gpt2\",\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"architectures\": [\n",
    "    \"GPT2LMHeadModel\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0\n",
    "  },\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_inner\": null,\n",
    "  \"n_layer\": 12,\n",
    "  \"n_positions\": 1024,\n",
    "  \"reorder_and_upcast_attn\": false,\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"scale_attn_by_inverse_layer_idx\": false,\n",
    "  \"scale_attn_weights\": true,\n",
    "  \"summary_activation\": null,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": true,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": true,\n",
    "  \"task_specific_params\": {\n",
    "    \"text-generation\": {\n",
    "      \"do_sample\": true,\n",
    "      \"max_length\": 50\n",
    "    }\n",
    "  },\n",
    "  \"transformers_version\": \"4.15.0\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 50257\n",
    "}\n",
    "```\n",
    "\n",
    "https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gpt2/configuration_gpt2.html\n",
    "\n",
    "summary is used for sequence compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75d7d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carson/projects/modelstates/hugface_models/gpt2\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2' # 'EleutherAI/gpt-j-6B' # 'coherence/gpt2-xl'\n",
    "param_path = local_hugface_directory+model_name\n",
    "print(param_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name,pad_token='<|endoftext|>')\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_save_path,pad_token='<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc5f3b",
   "metadata": {},
   "source": [
    "```\n",
    "GPT2LMHeadModel(\n",
    "  (transformer): GPT2Model(\n",
    "    (wte): Embedding(50257, 768)\n",
    "    (wpe): Embedding(1024, 768)\n",
    "    (drop): Dropout(p=0.1, inplace=False)\n",
    "    (h): ModuleList(\n",
    "      (0): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      .\n",
    "      12 repeating blocks\n",
    "      .\n",
    "      (11): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a154ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.4.attn.masked_bias', 'v_head.summary.bias', 'lm_head.weight', 'h.0.attn.masked_bias', 'h.3.attn.masked_bias', 'h.1.attn.masked_bias', 'v_head.summary.weight', 'h.9.attn.masked_bias', 'h.5.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.10.attn.masked_bias', 'h.6.attn.masked_bias', 'h.11.attn.masked_bias', 'h.2.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params 124440577\n",
      "did not place model on any GPUs\n"
     ]
    }
   ],
   "source": [
    "model = GPT2HeadWithValueModel.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir = param_path,\n",
    ")\n",
    "\n",
    "'''\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir = param_path,\n",
    ")\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\n",
    "    model_save_path,\n",
    "    revision=\"float16\", \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"num_params\", sum(p.numel() for p in model.parameters() if p.requires_grad)) \n",
    "\n",
    "if NUM_GPUS == 1:\n",
    "    print('model = model.cuda()')\n",
    "    model = model.cuda()\n",
    "elif NUM_GPUS > 1:\n",
    "    # break up model and place model components on different GPUs\n",
    "    print('model.parallelize()')\n",
    "    model.parallelize()\n",
    "else:\n",
    "    print('did not place model on any GPUs')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2d714",
   "metadata": {},
   "source": [
    "expected output from adding additional value head\n",
    "\n",
    "\n",
    "\n",
    "class GPT2HeadWithValueModel(GPT2PreTrainedModel):\n",
    "\n",
    "warning\n",
    "\n",
    "```\n",
    "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.6.attn.masked_bias', 'h.10.attn.masked_bias', 'h.0.attn.masked_bias', 'h.3.attn.masked_bias', 'h.7.attn.masked_bias', 'h.5.attn.masked_bias', 'h.11.attn.masked_bias', 'h.9.attn.masked_bias', 'h.8.attn.masked_bias', 'lm_head.weight', 'h.4.attn.masked_bias', 'v_head.summary.weight', 'h.2.attn.masked_bias', 'v_head.summary.bias', 'h.1.attn.masked_bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "model\n",
    "```\n",
    "GPT2HeadWithValueModel(\n",
    "  (transformer): GPT2Model(\n",
    "    (wte): Embedding(50257, 768)\n",
    "    (wpe): Embedding(1024, 768)\n",
    "    (drop): Dropout(p=0.1, inplace=False)\n",
    "    (h): ModuleList(\n",
    "      (0): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      .\n",
    "      12 repeating blocks\n",
    "      .\n",
    "      (11): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
    "  (v_head): ValueHead(\n",
    "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
    "    (activation): Identity()\n",
    "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
    "    (last_dropout): Identity()\n",
    "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "class GPT2HeadWithValueModel(GPT2LMHeadModel):\n",
    "\n",
    "warning\n",
    "\n",
    "```\n",
    "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['v_head.summary.bias', 'v_head.summary.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "\n",
    "model\n",
    "\n",
    "```\n",
    "GPT2HeadWithValueModel(\n",
    "  (transformer): GPT2Model(\n",
    "    (wte): Embedding(50257, 768)\n",
    "    (wpe): Embedding(1024, 768)\n",
    "    (drop): Dropout(p=0.1, inplace=False)\n",
    "    (h): ModuleList(\n",
    "      (0): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (act): NewGELUActivation()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      .\n",
    "      12 repeating blocks\n",
    "      .\n",
    "      (11): GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2Attention(\n",
    "          (c_attn): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D()\n",
    "          (c_proj): Conv1D()\n",
    "          (act): NewGELUActivation()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
    "  (v_head): ValueHead(\n",
    "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
    "    (activation): Identity()\n",
    "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
    "    (last_dropout): Identity()\n",
    "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "  )\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c2247e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"I liked the movie Transformers!\" + tokenizer.eos_token\n",
    "input_ids = tokenizer.encode(input_txt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "logits, transformer_outputs, values = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "51df3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13e861ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> .\n",
      " liked -->  the\n",
      " the -->  idea\n",
      " movie --> ,\n",
      " Transformers --> ,\n",
      "! -->  I\n",
      "<|endoftext|> --> The\n"
     ]
    }
   ],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "for i in range(input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i])\n",
    "    print(current_id, '-->', next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7c52d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return input_ids[:, -txt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "204415a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 5]), torch.Size([1, 5])]\n",
      "My most favourite movie is Rodney Dangerfield, whose character is tooker Danny\n",
      "My least favourite movie is Spitfire. Luigi is an all-American comedic\n"
     ]
    }
   ],
   "source": [
    "query_txt_1 = \"My most favourite movie is\"\n",
    "query_txt_2 = \"My least favourite movie is\"\n",
    "queries_txt = [query_txt_1, query_txt_2]\n",
    "\n",
    "queries = [tokenizer.encode(query_txt, return_tensors=\"pt\") for query_txt in queries_txt]\n",
    "print([q.shape for q in queries])\n",
    "queries = torch.cat(queries)\n",
    "\n",
    "responses = respond_to_batch(model, queries, txt_len=10)\n",
    "\n",
    "for i in range(responses.shape[0]):\n",
    "    response_txt = tokenizer.decode(responses[i])\n",
    "    query_txt = queries_txt[i]\n",
    "    print(query_txt + response_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03700c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
